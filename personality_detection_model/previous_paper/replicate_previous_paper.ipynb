{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate previous paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This github repository](https://github.com/yashsmehta/personality-prediction#predicting-personality-on-unseen-text) contains code for the paper [Bottom-Up and Top-Down: Predicting Personality with Psycholinguistic and Language Model Features](https://ieeexplore.ieee.org/document/9338428), where the authors propose a novel deep learning-based model which integrates traditional psycholinguistic features with language model embeddings to predict personality from the Essays dataset for Big-Five and Kaggle dataset for MBTI.\n",
    "\n",
    "We used this paper's result as a baseline of model performance for personality detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'personality-prediction'...\n",
      "remote: Enumerating objects: 944, done.\u001b[K\n",
      "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
      "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
      "remote: Total 944 (delta 71), reused 108 (delta 67), pack-reused 826\u001b[K\n",
      "Receiving objects: 100% (944/944), 53.49 MiB | 35.54 MiB/s, done.\n",
      "Resolving deltas: 100% (564/564), done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Try this code only at first time\n",
    "\"\"\"\n",
    "# Clone the git hub repo onto my Google Drive working folder\n",
    "#!git clone 'https://github.com/yashsmehta/personality-prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yuta/Documents/GitHub/LLM-personality-evaluation/personality_detection_model/previous_paper/personality-prediction\n"
     ]
    }
   ],
   "source": [
    "%cd personality-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "df = pd.read_csv('personality-prediction/data/kaggle/kaggle.csv')\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=SEED)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv('data/kaggle/kaggle_train.csv', index=False)\n",
    "test_df.to_csv('data/kaggle/kaggle_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from text data using BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "!python LM_extractor.py -dataset_type 'kaggle' -token_length 512 -batch_size 32 -embed 'bert-base' -op_dir 'pkl_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune detection model (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python finetune_models/MLP_LM.py -dataset \"kaggle\" -save_model \"yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict personality on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified version of unseen_predictor.py\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#parent_dir = os.path.dirname(os.getcwd())\n",
    "#sys.path.insert(0, parent_dir)\n",
    "#sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# This line are needed only when run code on Colab\n",
    "sys.path.append('/content/drive/MyDrive/Capstone/personality-prediction')\n",
    "\n",
    "#import utils.gen_utils as utils\n",
    "import utils.dataset_processors as dataset_processors\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"GPU found (\", torch.cuda.get_device_name(torch.cuda.current_device()), \")\")\n",
    "    torch.cuda.set_device(torch.cuda.current_device())\n",
    "    print(\"num device avail: \", torch.cuda.device_count())\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Running on cpu\")\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "\n",
    "def get_bert_model(embed):\n",
    "    if embed == \"bert-base\":\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    elif embed == \"bert-large\":\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "        model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "    elif embed == \"albert-base\":\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "        model = BertModel.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "    elif embed == \"albert-large\":\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"albert-large-v2\")\n",
    "        model = BertModel.from_pretrained(\"albert-large-v2\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Unknown pre-trained model: {embed}! Aborting...\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def load_finetune_model(op_dir, finetune_model, dataset):\n",
    "    trait_labels = []\n",
    "\n",
    "    if dataset == \"kaggle\":\n",
    "        trait_labels = [\"E\", \"N\", \"F\", \"J\"]\n",
    "    else:\n",
    "        trait_labels = [\"EXT\", \"NEU\", \"AGR\", \"CON\", \"OPN\"]\n",
    "\n",
    "    path_model = op_dir + \"finetune_\" + str(finetune_model).lower()\n",
    "\n",
    "    if not Path(path_model).is_dir():\n",
    "        print(f\"The directory with the selected model was not found: {path_model}\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    def abort_if_model_not_exist(model_name):\n",
    "        if not Path(model_name).is_file():\n",
    "            print(\n",
    "                f\"Model not found: {model_name}. Either the model was not trained or the model name is incorrect! Aborting...\"\n",
    "            )\n",
    "            sys.exit(0)\n",
    "\n",
    "    models = {}\n",
    "    for trait in trait_labels:\n",
    "        if re.search(r\"MLP_LM\", str(finetune_model).upper()):\n",
    "            model_name = f\"{path_model}/MLP_LM_{trait}.h5\"\n",
    "#            print(f\"Load model: {model_name}\")\n",
    "            abort_if_model_not_exist(model_name)\n",
    "            model = tf.keras.models.load_model(model_name)\n",
    "\n",
    "        elif re.search(r\"SVM_LM\", str(finetune_model).upper()):\n",
    "            model_name = f\"{path_model}/SVM_LM_{trait}.pkl\"\n",
    "#            print(f\"Load model: {model_name}\")\n",
    "            abort_if_model_not_exist(model_name)\n",
    "            model = joblib.load(model_name)\n",
    "\n",
    "        else:\n",
    "            print(f\"Unknown finetune model: {model_name}! Aborting...\")\n",
    "            sys.exit(0)\n",
    "\n",
    "        models[trait] = model\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def extract_bert_features(text, tokenizer, model, token_length, overlap=256):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    n_tokens = len(tokens)\n",
    "\n",
    "    start, segments = 0, []\n",
    "    while start < n_tokens:\n",
    "        end = min(start + token_length, n_tokens)\n",
    "        segment = tokens[start:end]\n",
    "        segments.append(segment)\n",
    "        if end == n_tokens:\n",
    "            break\n",
    "        start = end - overlap\n",
    "\n",
    "    embeddings_list = []\n",
    "    with torch.no_grad():\n",
    "        for segment in segments:\n",
    "            inputs = tokenizer(\n",
    "                \" \".join(segment), return_tensors=\"pt\", padding=True, truncation=True\n",
    "            )\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings_list.append(embeddings)\n",
    "\n",
    "    if len(embeddings_list) > 1:\n",
    "        embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "        embeddings = np.mean(embeddings, axis=0, keepdims=True)\n",
    "    else:\n",
    "        embeddings = embeddings_list[0]\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def predict(\n",
    "        new_text,\n",
    "        dataset: str='essays',\n",
    "        token_length: int=512,\n",
    "        batch_size: int=32,\n",
    "        embed: str='bert-base',\n",
    "        op_dir: str='pkl_data/',\n",
    "        mode: str='512_head',\n",
    "        embed_mode: str='cls',\n",
    "        finetune_model: str='mlp_lm'\n",
    "    ):\n",
    "#    print(\n",
    "#        \"{} | {} | {} | {} | {} | {}\".format(\n",
    "#            dataset, embed, token_length, mode, embed_mode, finetune_model\n",
    "#        )\n",
    "#    )\n",
    "    new_text_pre = dataset_processors.preprocess_text(new_text)\n",
    "\n",
    "    tokenizer, model = get_bert_model(embed)\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    new_embeddings = extract_bert_features(new_text_pre, tokenizer, model, token_length)\n",
    "#    print(\"finetune model: \", finetune_model)\n",
    "    models, predictions = load_finetune_model(op_dir, finetune_model, dataset), {}\n",
    "\n",
    "    for trait, model in models.items():\n",
    "        try:\n",
    "            #prediction = model.predict(new_embeddings)\n",
    "            prediction = model.predict(new_embeddings, verbose=0) # add verbose=0\n",
    "            prediction = softmax(prediction)\n",
    "            prediction = prediction[0][1]\n",
    "\n",
    "            # find the index of the highest probability (predicted class)\n",
    "            predictions[trait] = prediction  # get the probability of yes\n",
    "\n",
    "        except BaseException as e:\n",
    "            print(f\"Failed to make prediction: {e}\")\n",
    "\n",
    "#    print(f\"\\nPersonality predictions using {str(finetune_model).upper()}:\")\n",
    "#    for trait, prediction in predictions.items():\n",
    "#        binary_prediction = \"Yes\" if prediction > 0.5 else \"No\"\n",
    "#        print(f\"{trait}: {binary_prediction}: {prediction:.3f}\")\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "def labeling(pred_dict):\n",
    "    label = \"\"\n",
    "    label += \"E\" if pred_dict['E'] >= 0.5 else \"I\"\n",
    "    label += \"N\" if pred_dict['N'] >= 0.5 else \"S\"\n",
    "    label += \"F\" if pred_dict['F'] >= 0.5 else \"T\"\n",
    "    label += \"J\" if pred_dict['J'] >= 0.5 else \"P\"\n",
    "    return label\n",
    "\n",
    "df_test = pd.read_csv('data/kaggle/kaggle_test.csv')\n",
    "\n",
    "preds = df_test['text'].progress_apply(lambda x: predict(x, dataset='kaggle'))\n",
    "pred_labels = preds.apply(lambda x: labeling(x))\n",
    "pred_labels.name = \"pred_label\"\n",
    "\n",
    "pred_labels.to_csv('explogs/test_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "ref_labels = pd.read_csv('data/kaggle/kaggle_test.csv')['type']\n",
    "pred_labels = pd.read_csv('explogs/test_prediction.csv')['pred_label']\n",
    "\n",
    "cor_dict = defaultdict(int)\n",
    "multi_cor_list = [] # list to store 1 if all dimension are correct, else 0\n",
    "\n",
    "for ref, pred in zip(ref_labels, pred_labels):\n",
    "    multi_cor = 1\n",
    "    for i in range(4):\n",
    "        if ref[i] == pred[i]:\n",
    "            cor_dict[i] += 1\n",
    "            multi_cor *= 1\n",
    "        else:\n",
    "            multi_cor *= 0\n",
    "\n",
    "    multi_cor_list.append(multi_cor)\n",
    "\n",
    "total = len(ref_labels)\n",
    "acc_dict = {idx: round(cor / total, 4) for idx, cor in cor_dict.items()}\n",
    "print(f\"Accuracy for each dimension: {acc_dict}\")\n",
    "\n",
    "multi_acc = round(sum(multi_cor_list) / total, 4)\n",
    "print(f\"Accuracy for 16 class classification: {multi_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
