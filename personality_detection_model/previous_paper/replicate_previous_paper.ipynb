{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHB7YFY12TN5"
      },
      "source": [
        "# Replicate previous paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ujaud_22TN6"
      },
      "source": [
        "[This github repository](https://github.com/yashsmehta/personality-prediction#predicting-personality-on-unseen-text) contains code for the paper [Bottom-Up and Top-Down: Predicting Personality with Psycholinguistic and Language Model Features](https://ieeexplore.ieee.org/document/9338428), where the authors propose a novel deep learning-based model which integrates traditional psycholinguistic features with language model embeddings to predict personality from the Essays dataset for Big-Five and Kaggle dataset for MBTI.\n",
        "\n",
        "We used this paper's result as a baseline of model performance for personality detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f2IgNEt2TN6",
        "outputId": "a4400c62-1719-4c35-d5b0-b24df7971ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'personality-prediction'...\n",
            "remote: Enumerating objects: 962, done.\u001b[K\n",
            "remote: Counting objects: 100% (962/962), done.\u001b[K\n",
            "remote: Compressing objects: 100% (364/364), done.\u001b[K\n",
            "remote: Total 962 (delta 582), reused 945 (delta 580), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (962/962), 53.49 MiB | 6.10 MiB/s, done.\n",
            "Resolving deltas: 100% (582/582), done.\n",
            "Updating files: 100% (58/58), done.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Try this code only at first time\n",
        "\"\"\"\n",
        "# Clone the git hub repo onto my Google Drive working folder\n",
        "#!git clone 'https://github.com/Yuta555/personality-prediction'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNG8kbD3-A6p",
        "outputId": "468d7a49-af25-4b38-d8b5-6a4aae8ae97d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers tweet-preprocessor sentencepiece python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fxnYvE492TN7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUawPaB02TN7",
        "outputId": "557df85b-967e-4294-bf11-4bf6771120a9"
      },
      "outputs": [],
      "source": [
        "%cd personality-prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCOFyQLZ2TN7"
      },
      "source": [
        "## Split data into train/test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qab-Z8Q12TN7"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "df = pd.read_csv('data/kaggle/kaggle.csv')\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.1, random_state=SEED)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "train_df.to_csv('data/kaggle/kaggle_train.csv', index=False)\n",
        "test_df.to_csv('data/kaggle/kaggle_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tklByLZE2TN7"
      },
      "source": [
        "## Extract features from text data using BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XbEXNRu2TN8",
        "outputId": "d4d13e5b-2e42-4713-817c-23bd3cf6d90d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_flax_utils.py:24: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import FlaxGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "GPU found ( Tesla T4 )\n",
            "num device avail:  1\n",
            "\n",
            "kaggle | bert-base | 512 | 512_head | cls\n",
            "\n",
            "Downloading (…)lve/main/config.json: 100% 570/570 [00:00<00:00, 3.04MB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Downloading model.safetensors: 100% 440M/440M [00:01<00:00, 238MB/s]\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/model.safetensors\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 129kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 3.32MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 3.53MB/s]\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/tokenizer_config.json\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/tokenizer.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "E :  0    5993\n",
            "1    1814\n",
            "Name: E, dtype: int64\n",
            "N :  1    6731\n",
            "0    1076\n",
            "Name: N, dtype: int64\n",
            "F :  1    4229\n",
            "0    3578\n",
            "Name: F, dtype: int64\n",
            "J :  0    4705\n",
            "1    3102\n",
            "Name: J, dtype: int64\n",
            "[\"'\", 'i', \"'\", 'm', 'always', 'starting', 'new', 'books', ',', 'so']\n",
            "[\"'\", 'ah', 'cool', '!', 'yeah', ',', 'i', \"'\", 'm', 'not']\n",
            "[\"'\", 'yes', ',', 'i', \"'\", 'm', 'and', 'i', \"'\", 'm']\n",
            "[\"'\", 'i', 'love', 'john', 'lennon', \"'\", 's', 'imagine', 'song', '.']\n",
            "[\"'\", 'the', 'only', 'men', 'i', 'liked', 'in', 'life', '(', 'in']\n",
            "[\"'\", 'when', 'my', 'friend', 'was', 'the', 'target', 'of', 'an', 'attempted']\n",
            "[\"'\", 'there', 'are', 'probably', 'too', 'many', 'songs', 'that', 'define', 'my']\n",
            "[\"'\", 'let', \"'\", 's', 'see', ')', 'god', ')', 'piano', ')']\n",
            "[\"'\", '8', 'year', 'old', 'en', '##tp', ':', 'dunn', '##o', ',']\n",
            "[\"'\", 'any', 'thoughts', '/', 'insights', 'on', 'an', 'in', '##f', '##j']\n",
            "average length :  1729\n",
            "\n",
            "gpu mem alloc:  0.47  GB\n",
            "starting to extract LM embeddings...\n",
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "extracting embeddings for kaggle dataset: DONE!\n"
          ]
        }
      ],
      "source": [
        "!python LM_extractor.py -dataset_type 'kaggle' -token_length 512 -batch_size 32 -embed 'bert-base' -op_dir 'pkl_data' -kaggle_train True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-tiRp9x2TN8"
      },
      "source": [
        "## Fine-tune detection model (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PmGtuDw2TN8",
        "outputId": "6fb09213-60dc-4bb3-f504-9165fd20904e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-11-12 23:09:06.877705: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-12 23:09:06.877753: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-12 23:09:06.877795: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "kaggle : bert-base : 11 : 512_head : cls\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "{'acc': [76.82458162307739, 77.59283185005188, 77.08066701889038, 77.46478915214539, 76.69654488563538, 77.20870971679688, 77.20870971679688, 76.92307829856873, 77.43589878082275, 76.92307829856873, 86.29961609840393, 86.17157340049744, 86.17157340049744, 86.17157340049744, 86.17157340049744, 86.17157340049744, 86.29961609840393, 86.28205060958862, 86.28205060958862, 86.28205060958862, 69.91037130355835, 69.14212703704834, 67.60563254356384, 71.44686579704285, 69.78232860565186, 71.57490253448486, 70.80665826797485, 69.87179517745972, 68.84615421295166, 67.94871687889099, 61.843788623809814, 61.331623792648315, 63.12419772148132, 63.38028311729431, 62.0998740196228, 62.35595345497131, 62.74007558822632, 60.89743375778198, 61.538463830947876, 61.92307472229004], 'trait': ['E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J', 'J'], 'fold': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
            "         acc trait  fold network dataset      lr  batch_size  epochs  \\\n",
            "0  76.824582     E     1     MLP  kaggle  0.0005          32      10   \n",
            "1  77.592832     E     2     MLP  kaggle  0.0005          32      10   \n",
            "2  77.080667     E     3     MLP  kaggle  0.0005          32      10   \n",
            "3  77.464789     E     4     MLP  kaggle  0.0005          32      10   \n",
            "4  76.696545     E     5     MLP  kaggle  0.0005          32      10   \n",
            "\n",
            "   model_input      embed layer      mode embed_mode  jobid  \n",
            "0  LM_features  bert-base    11  512_head        cls      0  \n",
            "1  LM_features  bert-base    11  512_head        cls      0  \n",
            "2  LM_features  bert-base    11  512_head        cls      0  \n",
            "3  LM_features  bert-base    11  512_head        cls      0  \n",
            "4  LM_features  bert-base    11  512_head        cls      0  \n"
          ]
        }
      ],
      "source": [
        "!python finetune_models/MLP_LM.py -dataset \"kaggle\" -save_model \"yes\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQWYeJd32TN8"
      },
      "source": [
        "## Predict personality on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaERE_Oj7Qlo"
      },
      "outputs": [],
      "source": [
        "#%cd personality-prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoD3F9g64-Lp",
        "outputId": "ae075ec5-ddac-4597-878e-b1a182b8c8d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 868/868 [36:02<00:00,  2.49s/it]\n"
          ]
        }
      ],
      "source": [
        "from .unseen_predictor import predict\n",
        "\n",
        "embed = \"bert-base\"\n",
        "op_dir = \"pkl_data/\"\n",
        "token_length = 512\n",
        "finetune_model = \"mlp_lm\"\n",
        "dataset = \"kaggle\"\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "def labeling(pred_dict):\n",
        "    label = \"\"\n",
        "    label += \"E\" if pred_dict['E'] >= 0.5 else \"I\"\n",
        "    label += \"N\" if pred_dict['N'] >= 0.5 else \"S\"\n",
        "    label += \"F\" if pred_dict['F'] >= 0.5 else \"T\"\n",
        "    label += \"J\" if pred_dict['J'] >= 0.5 else \"P\"\n",
        "    return label\n",
        "\n",
        "df_test = pd.read_csv('data/kaggle/kaggle_test.csv')\n",
        "\n",
        "preds = df_test['text'].progress_apply(lambda x: predict(x, embed, op_dir, token_length, finetune_model, dataset))\n",
        "pred_labels = preds.apply(lambda x: labeling(x))\n",
        "pred_labels.name = \"pred_label\"\n",
        "\n",
        "pred_labels.to_csv('explogs/test_prediction.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_-K2qU22TN8"
      },
      "source": [
        "## Evaluate the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Edm56LwS2TN8",
        "outputId": "b6b61269-df0f-4622-daa8-09844ce4d59b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for each dimension: {0: 0.7247, 1: 0.8606, 2: 0.7166, 3: 0.621}\n",
            "Accuracy for 16 class classification: 0.2938\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "ref_labels = pd.read_csv('data/kaggle/kaggle_test.csv')['type']\n",
        "pred_labels = pd.read_csv('explogs/test_prediction.csv')['pred_label']\n",
        "\n",
        "cor_dict = defaultdict(int)\n",
        "multi_cor_list = [] # list to store 1 if all dimension are correct, else 0\n",
        "\n",
        "for ref, pred in zip(ref_labels, pred_labels):\n",
        "    multi_cor = 1\n",
        "    for i in range(4):\n",
        "        if ref[i] == pred[i]:\n",
        "            cor_dict[i] += 1\n",
        "            multi_cor *= 1\n",
        "        else:\n",
        "            multi_cor *= 0\n",
        "\n",
        "    multi_cor_list.append(multi_cor)\n",
        "\n",
        "total = len(ref_labels)\n",
        "acc_dict = {idx: round(cor / total, 4) for idx, cor in sorted(cor_dict.items())}\n",
        "print(f\"Accuracy for each dimension: {acc_dict}\")\n",
        "\n",
        "multi_acc = round(sum(multi_cor_list) / total, 4)\n",
        "print(f\"Accuracy for 16 class classification: {multi_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esJKrlyjBhTC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
